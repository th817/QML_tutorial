{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Margin Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial shows usage of Multiclass Margin Classifier to classify Iris data of 4 features into 3 classes. It uses multiple one vs all classifier to do the task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "num_classes = 3\n",
    "margin = 0.15\n",
    "feature_size = 4\n",
    "batch_size = 10\n",
    "lr_adam = 0.01\n",
    "train_split = 0.75\n",
    "\n",
    "num_qubits = int(np.ceil(np.log2(feature_size)))\n",
    "num_layers = 6\n",
    "total_iterations = 100\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires = num_qubits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off by loading iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "def load_and_process_data():\n",
    "    iris = load_iris()\n",
    "    X, Y = iris.data, iris.target\n",
    "    X_torch = torch.tensor(X, requires_grad = False)\n",
    "    Y_torch = torch.tensor(Y, requires_grad = False)\n",
    "    print(\"First X Sample: \", X_torch[0])\n",
    "    \n",
    "    normalization = torch.sqrt(torch.sum(X_torch ** 2, dim = 1))\n",
    "    X_norm = X_torch / normalization.reshape(len(X_torch), 1)\n",
    "    print(\"First X norm sample: \", X_norm[0])\n",
    "\n",
    "    return X_norm, Y_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First X Sample:  tensor([5.1000, 3.5000, 1.4000, 0.2000], dtype=torch.float64)\n",
      "First X norm sample:  tensor([0.8038, 0.5516, 0.2206, 0.0315], dtype=torch.float64)\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "X, Y = load_and_process_data()\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we construct a multiclass classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(W):\n",
    "    for i in range(num_qubits):\n",
    "        qml.Rot(*W[i], wires = i)\n",
    "    for j in range(num_qubits - 1):\n",
    "        qml.CNOT(wires = [j, j + 1])\n",
    "    if num_qubits >= 2:\n",
    "        qml.CNOT(wires = [num_qubits - 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pennylane.templates.embeddings import AmplitudeEmbedding\n",
    "\n",
    "def circuit(weights, feat = None):\n",
    "    AmplitudeEmbedding(feat, [0,1], pad = 0.0, normalize = True)\n",
    "    for W in weights:\n",
    "        layer(W)\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "qnodes = []\n",
    "# we create variational classifiers for each classes\n",
    "for iq in range(num_classes):\n",
    "    qnode = qml.QNode(circuit, dev, interface = \"torch\")\n",
    "    qnodes.append(qnode)\n",
    "    \n",
    "    \n",
    "def variational_classifier(q_circuit, params, feat):\n",
    "    weights = params[0]\n",
    "    bias = params[1]\n",
    "    return q_circuit(weights, feat = feat) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_svm_loss(q_circuits, all_params, feature_vecs, true_labels):\n",
    "    loss = 0\n",
    "    num_samples = len(true_labels)\n",
    "    for i, feature_vec in enumerate(feature_vecs):\n",
    "        s_true = variational_classifier(\n",
    "            q_circuits[int(true_labels[i])], (all_params[0][int(true_labels[i])], all_params[1][int(true_labels[i])]), feature_vec)\n",
    "        s_true = s_true.float()\n",
    "        li = 0\n",
    "        \n",
    "        for j in range(num_classes):\n",
    "            if j != int(true_labels[i]):\n",
    "                s_j = variational_classifier(\n",
    "                q_circuits[j], (all_params[0][j], all_params[1][j]), feature_vec)\n",
    "                s_j = s_j.float()\n",
    "                li += torch.max(torch.zeros(1).float(), s_j - s_true + margin)\n",
    "        loss += li\n",
    "    return loss / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(q_circuits, all_params, feature_vecs, labels):\n",
    "    predicted_labels = []\n",
    "    for i, feature_vec in enumerate(feature_vecs):\n",
    "        scores = np.zeros(num_classes)\n",
    "        for c in range(num_classes):\n",
    "            score = variational_classifier(\n",
    "                q_circuits[c], (all_params[0][c], all_params[1][c]), feature_vec)\n",
    "            scores[c] = float(score)\n",
    "        pred_class = np.argmax(scores)\n",
    "        predicted_labels.append(pred_class)\n",
    "    return predicted_labels\n",
    "\n",
    "def accuracy(labels, hard_predictions):\n",
    "    loss = 0\n",
    "    for l,p in zip(labels, hard_predictions):\n",
    "        if np.abs(1 - p) < 1e-3:\n",
    "            loss = loss + 1\n",
    "    loss = loss / labels.shape[0]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def training(features, Y):\n",
    "    num_data = Y.shape[0]\n",
    "    feat_vecs_train, feat_vecs_test, Y_train, Y_test = train_test_split(features, Y)\n",
    "    num_train = Y_train.shape[0]\n",
    "    q_circuits = qnodes\n",
    "    \n",
    "    all_weights = [\n",
    "        Variable(0.1 * torch.randn(num_layers, num_qubits, 3), requires_grad = True)\n",
    "        for i in range(num_classes)\n",
    "    ]\n",
    "    all_bias = [Variable(0.1 * torch.ones(1), requires_grad = True) for i in range (num_classes)]\n",
    "    \n",
    "    optimizer = optim.Adam(all_weights + all_bias, lr = lr_adam)\n",
    "    params = (all_weights, all_bias)\n",
    "    print(\"Num params: \", 3 * num_layers * num_qubits * 3 + 3)\n",
    "    \n",
    "    costs, train_acc, test_acc = [], [], []\n",
    "    \n",
    "    for it in range(total_iterations):\n",
    "        batch_index = np.random.randint(0, num_train, (batch_size,))\n",
    "        feat_vecs_train_batch = feat_vecs_train[batch_index]\n",
    "        Y_train_batch = Y_train[batch_index]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        curr_cost = multiclass_svm_loss(q_circuits, params, feat_vecs_train_batch, Y_train_batch)\n",
    "        curr_cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        predictions_train = classify(q_circuits, params, feat_vecs_train, Y_train)\n",
    "        predictions_test = classify(q_circuits, params, feat_vecs_test, Y_test)\n",
    "        acc_train = accuracy(Y_train, predictions_train)\n",
    "        acc_test = accuracy(Y_test, predictions_test)\n",
    "        \n",
    "        if it % 10 == 0:\n",
    "            print(\"Iteration: \", it)\n",
    "            print(\"cost: \", curr_cost)\n",
    "            print(\"acc_train: \", acc_train)\n",
    "            print(\"acc_test: \", acc_test)\n",
    "    \n",
    "    return costs, train_acc, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First X Sample:  tensor([5.1000, 3.5000, 1.4000, 0.2000], dtype=torch.float64)\n",
      "First X norm sample:  tensor([0.8038, 0.5516, 0.2206, 0.0315], dtype=torch.float64)\n",
      "Num params:  111\n",
      "Iteration:  0\n",
      "cost:  tensor([0.3649], grad_fn=<DivBackward0>)\n",
      "acc_train:  0.32142857142857145\n",
      "acc_test:  0.34210526315789475\n",
      "Iteration:  10\n",
      "cost:  tensor([0.1440], grad_fn=<DivBackward0>)\n",
      "acc_train:  0.008928571428571428\n",
      "acc_test:  0.02631578947368421\n",
      "Iteration:  20\n",
      "cost:  tensor([0.1479], grad_fn=<DivBackward0>)\n",
      "acc_train:  0.6517857142857143\n",
      "acc_test:  0.6052631578947368\n",
      "Iteration:  30\n",
      "cost:  tensor([0.0290], grad_fn=<DivBackward0>)\n",
      "acc_train:  0.0\n",
      "acc_test:  0.0\n",
      "Iteration:  40\n",
      "cost:  tensor([0.0501], grad_fn=<DivBackward0>)\n",
      "acc_train:  0.6696428571428571\n",
      "acc_test:  0.6578947368421053\n",
      "Iteration:  50\n",
      "cost:  tensor([0.0668], grad_fn=<DivBackward0>)\n",
      "acc_train:  0.26785714285714285\n",
      "acc_test:  0.2894736842105263\n",
      "Iteration:  60\n",
      "cost:  tensor([0.0576], grad_fn=<DivBackward0>)\n",
      "acc_train:  0.026785714285714284\n",
      "acc_test:  0.05263157894736842\n",
      "Iteration:  70\n",
      "cost:  tensor([0.0709], grad_fn=<DivBackward0>)\n",
      "acc_train:  0.35714285714285715\n",
      "acc_test:  0.42105263157894735\n",
      "Iteration:  80\n",
      "cost:  tensor([0.0462], grad_fn=<DivBackward0>)\n",
      "acc_train:  0.30357142857142855\n",
      "acc_test:  0.3157894736842105\n",
      "Iteration:  90\n",
      "cost:  tensor([0.0294], grad_fn=<DivBackward0>)\n",
      "acc_train:  0.22321428571428573\n",
      "acc_test:  0.23684210526315788\n"
     ]
    }
   ],
   "source": [
    "features, Y = load_and_process_data()\n",
    "costs, train_acc, test_acc = training(features, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
