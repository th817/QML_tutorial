{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel-based training of quantum models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the last few years, it has been increaing clear that mathematical foundations of quantum models are intimately related to kernel-method. In this tutorial, I will use 2 methods of QML, well known variational quantum circuit and alternative kernel based circuit, to train for classic iris datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.templates import AngleEmbedding, StronglyEntanglingLayers\n",
    "from pennylane.operation import Tensor\n",
    "\n",
    "import torch\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y = True)\n",
    "X = X[:100]\n",
    "y = y[:100]\n",
    "\n",
    "scaler = StandardScaler().fit(X)\n",
    "\n",
    "X_scaled = scaler.transform(X)\n",
    "y_scaled = 2 * (y - 0.5)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = len(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our dataset ready, let's try original variational circuit approach to train for the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Variational Circuit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_var = qml.device(\"default.qubit\", wires = n_qubits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev_var, interface = \"torch\", diff_method=\"parameter-shift\")\n",
    "def quantum_model(x, params):\n",
    "    AngleEmbedding(x, wires = range(n_qubits))\n",
    "    StronglyEntanglingLayers(params, wires = range(n_qubits))\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "def quantum_model_plus_bias(x, params, bias):\n",
    "    return quantum_model(x, params) + bias\n",
    "\n",
    "def hinge_loss(predictions, targets):\n",
    "    all_ones = torch.ones_like(targets)\n",
    "    hinge_loss = all_ones - predictions * targets\n",
    "    hinge_loss = relu(hinge_loss)\n",
    "    return hinge_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantum_model_train(n_layers, steps, batch_size):\n",
    "    params = np.random.random((n_layers, n_qubits, n_qubits - 1))\n",
    "    params_torch = torch.tensor(params, requires_grad = True)\n",
    "    bias_torch = torch.tensor(0.0)\n",
    "    \n",
    "    opt = torch.optim.Adam([params_torch, bias_torch], lr = 0.1)\n",
    "    loss_history = []\n",
    "    \n",
    "    for i in range(steps):\n",
    "        batch_ids = np.random.choice(len(X_train), batch_size)\n",
    "        \n",
    "        X_batch = X_train[batch_ids]\n",
    "        y_batch = y_train[batch_ids]\n",
    "        \n",
    "        X_batch_torch = torch.tensor(X_batch, requires_grad = False)\n",
    "        y_batch_torch = torch.tensor(y_batch, requires_grad = False)\n",
    "        \n",
    "        def closure():\n",
    "            opt.zero_grad()\n",
    "            preds = torch.stack(\n",
    "            [quantum_model_plus_bias(x, params_torch, bias_torch) for x in X_batch_torch])\n",
    "            loss = torch.mean(hinge_loss(preds, y_batch_torch))\n",
    "            \n",
    "            current_loss = loss.detach().numpy().item()\n",
    "            loss_history.append(current_loss)\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                print(\"steps\", i, \", loss\", current_loss)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        \n",
    "        opt.step(closure)\n",
    "    return params_torch, bias_torch, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantum_model_predict(X_pred, trained_params, trained_bias):\n",
    "    preds = []\n",
    "    for x in X_test:\n",
    "        pred_torch = quantum_model_plus_bias(x, trained_params, trained_bias)\n",
    "        pred = pred_torch.detach().numpy().item()\n",
    "        if pred > 0:\n",
    "            pred = 1\n",
    "        else:\n",
    "            pred = -1\n",
    "            \n",
    "        preds.append(pred)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train our strongly entangled quantum circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps 0 , loss 1.1413586273404828\n",
      "steps 10 , loss 0.7847782655707439\n",
      "steps 20 , loss 0.8228469937468239\n",
      "steps 30 , loss 0.5551509120320679\n",
      "steps 40 , loss 0.5348194363360191\n",
      "steps 50 , loss 0.5459062454527552\n",
      "steps 60 , loss 0.48425058460965237\n"
     ]
    }
   ],
   "source": [
    "n_layers = 2\n",
    "batch_size = 20\n",
    "steps = 100\n",
    "trained_params, trained_bias, loss_history = quantum_model_train(n_layers, steps, batch_size)\n",
    "\n",
    "pred_test = quantum_model_predict(X_test, trained_params, trained_bias)\n",
    "print(\"accuracy on the test set: \", accuracy_score(pred_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.ylim((0,1))\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quantum circuit is successfully trained for the iris dataset. We can check the numbers of quantum function execution in the device,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_exe = dev_var.num_executions\n",
    "print(n_exe, \" numbers of quantum functions were executed during training process.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Kernel-based Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_ker = qml.device(\"default.qubit\", wires = n_qubits)\n",
    "\n",
    "projector = np.zeros((2**n_qubits, 2**n_qubits))\n",
    "projector[0, 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev_ker)\n",
    "def kernel(x1, x2):\n",
    "    AngleEmbedding(x1, wires = range(n_qubits))\n",
    "    qml.inv(AngleEmbedding(x2, wires = range(n_qubits)))\n",
    "    return qml.expval(qml.Hermitian(projector, wires = range(n_qubits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel(X_train[0], X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_matrix(A,B):\n",
    "    return np.array([[kernel(a,b) for b in B] for a in A])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel= kernel_matrix).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = svm.predict(X_test)\n",
    "accuracy_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_ker.num_executions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
