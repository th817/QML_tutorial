{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational Quantum Classifier is a quantum circuit that can be trained from original labelled data to classify new data samples. First we will classify simple parity function, then move on to more complicated iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import torch\n",
    "from pennylane import numpy as np\n",
    "from pennylane.optimize import NesterovMomentumOptimizer\n",
    "from pennylane.operation import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Classifying Parity function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parity function takes in 4 bits and returns 1(0) when there is odd(even) number of 1. We start by loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = [0. 0. 0. 0.], Y = -1\n",
      "X = [0. 0. 0. 1.], Y =  1\n",
      "X = [0. 0. 1. 0.], Y =  1\n",
      "X = [0. 0. 1. 1.], Y = -1\n",
      "X = [0. 1. 0. 0.], Y =  1\n",
      "X = [0. 1. 0. 1.], Y = -1\n",
      "X = [0. 1. 1. 0.], Y = -1\n",
      "X = [0. 1. 1. 1.], Y =  1\n",
      "X = [1. 0. 0. 0.], Y =  1\n",
      "X = [1. 0. 0. 1.], Y = -1\n",
      "X = [1. 0. 1. 0.], Y = -1\n",
      "X = [1. 0. 1. 1.], Y =  1\n",
      "X = [1. 1. 0. 0.], Y = -1\n",
      "X = [1. 1. 0. 1.], Y =  1\n",
      "X = [1. 1. 1. 0.], Y =  1\n",
      "X = [1. 1. 1. 1.], Y = -1\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(\"parity.txt\")\n",
    "X = np.array(data[:,:-1])\n",
    "Y = np.array(data[:, -1])\n",
    "Y = Y * 2 - np.ones(len(Y))\n",
    "\n",
    "\n",
    "for i in range(len(X)):\n",
    "    print('X = {}, Y = {: d}'.format(X[i], int(Y[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare variational circuit. Classical data input(X) is embedded using BasisEmbedding method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "dev = qml.device(\"default.qubit\", wires = n_qubits)\n",
    "def statepreparation(x):\n",
    "    qml.BasisState(x, wires = [0,1,2,3])\n",
    "\n",
    "def layer(W):\n",
    "    qml.Rot(*W[0], wires = 0)\n",
    "    qml.Rot(*W[1], wires = 1)\n",
    "    qml.Rot(*W[2], wires = 2)\n",
    "    qml.Rot(*W[3], wires = 3)\n",
    "    \n",
    "    qml.CNOT(wires = [0,1])\n",
    "    qml.CNOT(wires = [1,2])\n",
    "    qml.CNOT(wires = [2,3])\n",
    "    qml.CNOT(wires = [3,0])\n",
    "\n",
    "@qml.qnode(dev, interface = \"torch\", diff_method = \"parameter-shift\")\n",
    "def circuit(weights, x):\n",
    "    statepreparation(x)\n",
    "    for W in weights:\n",
    "        layer(W)    \n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add bias in a classical function,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Variational_classifier(weights, bias, x):\n",
    "    return circuit(weights, x) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss function (used square loss in this problem) and accuracy test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss(labels, predictions):\n",
    "    loss = 0\n",
    "    for i in range(len(labels)):\n",
    "        loss += (predictions[i] - labels[i])**2\n",
    "    loss / len(labels)\n",
    "    return loss\n",
    "\n",
    "def accuracy(labels, predictions):\n",
    "    accuracy = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        if abs(l - p) < 1e-2:\n",
    "            accuracy = accuracy + 1\n",
    "    accuracy = accuracy /len(labels)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we optimize the circuit,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "num_qubits = 4\n",
    "num_layers = 2\n",
    "steps = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parity_circuit_optimize(num_layers, steps):\n",
    "    \n",
    "    weights = 0.01 * np.random.randn (num_layers, num_qubits, 3)\n",
    "    weights_torch = torch.tensor(weights, requires_grad = True)\n",
    "    bias_torch = torch.tensor(0.0)\n",
    "    \n",
    "    loss_history = []\n",
    "    opt = torch.optim.Adam([weights_torch, bias_torch], lr = 0.1)\n",
    "    batch_size = 5\n",
    "    \n",
    "    for i in range(steps):\n",
    "        \n",
    "        batch_index = np.random.choice(len(X), size = batch_size)\n",
    "        \n",
    "        X_batch = X[batch_index]\n",
    "        Y_batch = Y[batch_index]\n",
    "        \n",
    "        X_batch_torch = torch.tensor(X_batch, requires_grad = False)\n",
    "        Y_batch_torch = torch.tensor(Y_batch, requires_grad = False)\n",
    "        \n",
    "        \n",
    "        opt.zero_grad()\n",
    "        predictions = torch.stack([Variational_classifier(weights_torch, bias_torch, x) for x in X_batch_torch])\n",
    "        loss = square_loss(predictions, Y_batch_torch)\n",
    "        current_loss = loss.detach().numpy().item()\n",
    "        loss_history.append(current_loss)\n",
    "        loss.backward()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print('steps ', i, ', loss', current_loss)\n",
    "        \n",
    "        opt.step()\n",
    "        \n",
    "    return weights_torch, bias_torch, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps  0 , loss 11.99557642046123\n",
      "steps  10 , loss 3.94989016832672\n",
      "steps  20 , loss 0.1701052593780992\n",
      "steps  30 , loss 0.17045122480922842\n",
      "steps  40 , loss 0.05683354087324432\n",
      "steps  50 , loss 0.0026986061264351155\n",
      "steps  60 , loss 0.0012003070329231025\n",
      "steps  70 , loss 0.0010615553884226964\n",
      "steps  80 , loss 0.0006776307642825416\n",
      "steps  90 , loss 0.0003918515351881998\n",
      "steps  100 , loss 0.00022525449654794104\n",
      "steps  110 , loss 0.0001232940370053039\n",
      "steps  120 , loss 8.355328400711438e-05\n",
      "steps  130 , loss 5.6041626573768125e-05\n",
      "steps  140 , loss 4.053508750437118e-05\n",
      "steps  150 , loss 3.2859187626222086e-05\n",
      "steps  160 , loss 2.7077865321274045e-05\n",
      "steps  170 , loss 2.0956197304874103e-05\n",
      "steps  180 , loss 1.6496512550007952e-05\n",
      "steps  190 , loss 1.5388608134770693e-05\n"
     ]
    }
   ],
   "source": [
    "weights_torch_trained, bias_torch_trained, loss_history = parity_circuit_optimize(num_layers, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9983,  0.9985,  0.9985, -0.9983,  0.9985, -0.9983, -0.9983,  0.9985,\n",
      "         0.9983, -0.9985, -0.9985,  0.9983, -0.9985,  0.9983,  0.9983, -0.9985],\n",
      "       dtype=torch.float64, grad_fn=<StackBackward>)\n",
      "tensor([-1.,  1.,  1., -1.,  1., -1., -1.,  1.,  1., -1., -1.,  1., -1.,  1.,\n",
      "         1., -1.], dtype=torch.float64)\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "X_torch = torch.tensor(X, requires_grad = False)\n",
    "Y_torch = torch.tensor(Y, requires_grad = False)\n",
    "\n",
    "predictions = torch.stack([Variational_classifier(weights_torch_trained, bias_torch_trained, x) for x in X_torch])\n",
    "print(predictions)\n",
    "print(Y_torch)\n",
    "acc = accuracy(Y_torch, predictions)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Iris Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qml.device(\"default.qubit\", wires = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First start by loading the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"iris.txt\")\n",
    "X = data[:,0:4]\n",
    "\n",
    "\n",
    "normalization = np.sqrt(np.sum(X**2, -1))\n",
    "X = (X.T / normalization).T\n",
    "\n",
    "\n",
    "Y = data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X[:, 0][Y == 1], X[:, 1][Y == 1], c=\"b\", marker=\"o\", edgecolors=\"k\")\n",
    "plt.scatter(X[:, 0][Y == -1], X[:, 1][Y == -1], c=\"r\", marker=\"o\", edgecolors=\"k\")\n",
    "plt.title(\"Original data\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Iris dataset, we are going to separate training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32963426 0.20602141 0.82408564 0.41204282] 1.0\n",
      "75\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)\n",
    "print(X_train[0], Y_train[0])\n",
    "print(len(X_train))\n",
    "print(len(Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Data Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use amplitude embedding for this problem. Data can be embedded into amplitudes of the quantum state with following procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angle(x):\n",
    "    \n",
    "    beta0 = 2 * np.arcsin(np.sqrt(x[1] ** 2)) / np.sqrt(x[0] ** 2 + x[1] ** 2 + 1e-12)\n",
    "    beta1 = 2 * np.arcsin(np.sqrt(x[3] ** 2)) / np.sqrt(x[2] ** 2 + x[3] ** 2 + 1e-12)\n",
    "    beta2 = 2 * np.arcsin(np.sqrt(x[2] ** 2 + x[3] **2) / np.sqrt(x[0] ** 2 + x[1] ** 2 + x[2] **2 + x[3] **2))\n",
    "    \n",
    "    return np.array([beta2, -beta1 / 2, beta1 / 2, -beta0 / 2, beta0 / 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statepreparation(a):\n",
    "    qml.RY(a[0], wires = 0)\n",
    "    \n",
    "    qml.CNOT(wires = [0,1])\n",
    "    qml.RY(a[1], wires = 1)\n",
    "    qml.CNOT(wires = [0,1])\n",
    "    qml.RY(a[2], wires = 1)\n",
    "    \n",
    "    qml.PauliX(wires = 0)\n",
    "    qml.CNOT(wires = [0,1])\n",
    "    qml.RY(a[3], wires = 1)\n",
    "    qml.CNOT(wires = [0,1])\n",
    "    qml.RY(a[4], wires = 1)\n",
    "    qml.PauliX(wires = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.55341699, 0.78513215, 0.27802363, 0.        ], requires_grad=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([0.539, 0.795, 0.278, 0.0])\n",
    "ang = get_angle(x)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def test(angles):\n",
    "    statepreparation(angles)\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "test(ang)\n",
    "np.real(dev.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or built in pennylane embedding templates can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pennylane.templates.embeddings import AmplitudeEmbedding\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def test(x):\n",
    "    AmplitudeEmbedding(x, wires = [0,1], normalize=True)\n",
    "    return qml.expval(qml.PauliZ(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.53904582, 0.79506758, 0.27802363, 0.        ], requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(x)\n",
    "np.real(dev.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Variational Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(W):\n",
    "    qml.Rot(*W[0], wires = 0)\n",
    "    qml.Rot(*W[1], wires = 1)\n",
    "    qml.CNOT(wires = [0,1])\n",
    "    \n",
    "@qml.qnode(dev, interface = 'torch', diff_method = 'parameter-shift')\n",
    "def circuit(weights, x):\n",
    "    AmplitudeEmbedding(x, wires = [0,1], normalize = True)\n",
    "    \n",
    "    for W in weights:\n",
    "        layer(W)\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "def variational_classifier(weights, bias, x):\n",
    "    return circuit(weights, x) + bias\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we optimize the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.random.seed(0)\n",
    "num_layers = 6\n",
    "batch_size = 5\n",
    "steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "num_layers = 6\n",
    "batch_size = 10\n",
    "steps = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_classifier(steps, num_layers):\n",
    "    weights = np.random.randn(num_layers, 2, 3) * 0.01\n",
    "    weights_torch = torch.tensor(weights, requires_grad = True)\n",
    "    bias_torch = torch.tensor(0.0, requires_grad = True)\n",
    "    \n",
    "    loss_history = []\n",
    "    opt = torch.optim.Adam([weights_torch, bias_torch], lr = 0.1)\n",
    "    \n",
    "    for i in range(steps):\n",
    "        batch_index = np.random.choice(len(X_train), size = batch_size)\n",
    "        \n",
    "        X_batch = X_train[batch_index]\n",
    "        Y_batch = Y_train[batch_index]\n",
    "        \n",
    "        X_batch_torch = torch.tensor(X_batch, requires_grad = False)\n",
    "        Y_batch_torch = torch.tensor(Y_batch, requires_grad = False)\n",
    "        \n",
    "        def closure():\n",
    "            opt.zero_grad()\n",
    "            preds = torch.stack([variational_classifier(weights_torch, bias_torch, x) for x in X_batch_torch])\n",
    "            loss = square_loss(preds, Y_batch_torch)\n",
    "            current_loss = loss.detach().numpy().item()\n",
    "            if i % 10 == 0:\n",
    "                print(\"steps \", i, \" loss\", current_loss)\n",
    "            loss_history.append(loss)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        \n",
    "        opt.step(closure)\n",
    "    return weights_torch, bias_torch, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps  0  loss 30.457022072088026\n",
      "steps  10  loss 1.1541648153736095\n",
      "steps  20  loss 0.9489440080299396\n",
      "steps  30  loss 0.7511141997659327\n",
      "steps  40  loss 0.8312739520510792\n",
      "steps  50  loss 0.41526550491704706\n",
      "steps  60  loss 0.7224791468807037\n",
      "steps  70  loss 0.5103029988468331\n",
      "steps  80  loss 0.6029966508505047\n",
      "steps  90  loss 0.6152051275700279\n",
      "steps  100  loss 0.5318072510090803\n",
      "steps  110  loss 0.589220421973285\n",
      "steps  120  loss 0.5826051311527691\n",
      "steps  130  loss 0.6221330082646681\n",
      "steps  140  loss 0.75577522119875\n"
     ]
    }
   ],
   "source": [
    "trained_weights_torch, trained_bias_torch, loss_history = optimize_classifier(steps, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_test(preds, Y):\n",
    "    accuracy = 0\n",
    "    for p,y in zip(preds, Y):\n",
    "        if abs(p - y) < 1e-3:\n",
    "            accuracy += 1\n",
    "    accuracy / len(Y)\n",
    "    return accuracy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.6881,  0.6098, -0.7748,  0.5840,  0.7257,  0.7826,  0.5814,  0.6412,\n",
      "        -0.8312,  0.8883, -0.8868,  0.5194, -0.6502, -0.9014,  0.6985, -0.1153,\n",
      "         0.6566,  0.6266,  0.7274, -0.6910,  0.6704, -0.9119, -0.8225, -0.9759,\n",
      "        -0.9631], dtype=torch.float64, grad_fn=<StackBackward>)\n",
      "tensor([ 1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1., -1., -1.,\n",
      "         1., -1.,  1.,  1.,  1., -1.,  1., -1., -1., -1., -1.],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "X_test_torch = torch.tensor(X_test)\n",
    "Y_test_torch = torch.tensor(Y_test)\n",
    "\n",
    "preds = torch.stack([variational_classifier(trained_weights_torch, trained_bias_torch, x)for x in X_test_torch])\n",
    "accuracy = accuracy_test(preds, Y_test_torch)\n",
    "print(preds)\n",
    "print(Y_test_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
